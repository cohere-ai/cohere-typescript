/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as Cohere from "../../../../index";

/**
 * @example
 *     {
 *         model: "model",
 *         messages: []
 *     }
 */
export interface V2ChatRequest {
    /** The model to use for the chat. */
    model: string;
    messages: Cohere.ChatMessages;
    tools?: Cohere.Tool2[];
    toolChoice?: Cohere.V2ChatRequestToolChoice;
    citationMode?: Cohere.V2ChatRequestCitationMode;
    truncationMode?: Cohere.V2ChatRequestTruncationMode;
    responseFormat?: Cohere.V2ChatRequestResponseFormat;
    /** The maximum number of tokens to generate. */
    maxTokens?: number;
    /** A list of strings that the model will stop generating at. */
    stopSequences?: string[];
    /** The maximum number of tokens to feed into the model. */
    maxInputTokens?: number;
    /** The temperature of the model. */
    temperature?: number;
    seed?: number;
    /** The frequency penalty of the model. */
    frequencyPenalty?: number;
    /** The presence penalty of the model. */
    presencePenalty?: number;
    k?: number;
    p?: number;
    /** Whether to return the prompt in the response. */
    returnPrompt?: boolean;
}
